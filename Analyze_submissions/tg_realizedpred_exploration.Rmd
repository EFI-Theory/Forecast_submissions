---
title: "theory"
author: "Caleb Robbins"
date: "7/22/2022"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


Ripping this code from Freya Olsson's tutorial: https://github.com/OlssonF/NEON-forecast-challenge-workshop/blob/main/Analyse_scores/Get_scores_tutorial.Rmd

```{r}
s3 <- arrow::s3_bucket(bucket = "neon4cast-scores/parquet/aquatics", endpoint_override= "data.ecoforecast.org")

start_ref_date <- as_date('2023-03-13') # what period do you want the scores for?
end_ref_date <- as_date('2023-08-13')
get_refdates <- as.character(seq(start_ref_date, end_ref_date, by = 'day'))

get_sites <- readr::read_csv("https://raw.githubusercontent.com/eco4cast/neon4cast-targets/main/NEON_Field_Site_Metadata_20220412.csv", show_col_types = F) |> 
  dplyr::filter(field_site_subtype == 'Wadeable Stream') |> # Subset to the lake sites
  select(field_site_id) |> slice_head(n = 5)|>
  pull() # get in a vector

get_variables <- c('temperature', 'oxygen', 'chla')

scores <- open_dataset(s3) |>
  filter(reference_datetime %in% get_refdates,
         str_detect(model_id, "tg_"),
         site_id %in% get_sites,
         variable %in% get_variables) |> 
  collect() |> 

  # Note: this can be slow so be patient 
  # The dataframe might also be very large depending on the filters you put in above
  mutate(horizon = as.numeric(as_date(datetime) - as_date(reference_datetime)))

```

```{r}
blah <- scores|>filter(model_id == "tg_randfor")|>filter(reference_datetime == "2023-04-10")

ggplot(data = blah, aes(x = horizon, y = mean-observation, color = variable))+
  geom_point()+
  facet_grid(.~site_id)

blah2<-blah|>group_by(horizon, )

#suppose what we want to do is aggregate scores for each forecast horizon

```

