---
title: "Pull data"
author: "Abby Lewis"
date: "2022-09-06"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
#install.packages("remotes")
#remotes::install_github("eco4cast/neon4cast")
library(neon4cast)
library(tidyverse)
library(lubridate)
library(tsibble)
library(fable)
#install.packages("rMR")
library(rMR)
library(arrow)
source("download_target.R")
```

NOTE: I have registered teams with this model for terrestrial daily, phenology, and aquatics


TO DO:
Deal with weekly timesteps
Set up for time series models
Currently running every site individually. Re-format to forecast across all?



Using a neon4cast example found here: https://github.com/eco4cast/neon4cast-example
One thing that is nice about this example is we can easily set it up to run automatically using code from Quinn

Describe team for EFI submission
```{r}
#Step 0: Define team name, team members, and theme

team_name <- "EFI Theory"

team_list <- list(list(individualName = list(givenName = "Abby", 
                                             surName = "Lewis"),
                       organizationName = "Virginia Tech",
                       electronicMailAddress = "aslewis@vt.edu")
                  )

model_id = "temp_lm"
model_themes = c("terrestrial_daily","aquatics","phenology") #This model is only relevant for three themes
model_types = c("terrestrial","aquatics","phenology") #Replace terrestrial daily and 30min with terrestrial
#Options: aquatics, beetles, phenology, terrestrial_30min, terrestrial_daily, ticks

model_metadata = list(
  forecast = list(
    model_description = list(
      forecast_model_id =  model_id, 
      type = "empirical",  
      repository = "https://github.com/abbylewis/EFI_Theory" 
    ),
    initial_conditions = list(
      status = "absent"
    ),
    drivers = list(
      status = "propagates",
      complexity = 1, #Just temperature
      propagation = list( 
        type = "ensemble", 
        size = 31) 
    ),
    parameters = list(
      status = "absent"
    ),
    random_effects = list(
      status = "absent"
    ),
    process_error = list(
      status = "absent"
    ),
    obs_error = list(
      status = "absent"
    )
  )
)



#Step 2: Get NOAA driver data
forecast_date <- Sys.Date()
noaa_date <- Sys.Date() - lubridate::days(1)  #Need to use yesterday's NOAA forecast because today's is not available yet

if("siteID" %in% colnames(target)){ #Sometimes the site is called siteID instead of site_id. Fixing here
  target = target%>%
    rename(site_id = siteID)
}
if("time" %in% colnames(target)){ #Sometimes the time column is instead labeled "datetime"
  target = target%>%
    rename(datetime = time)
}

#We're going to get data for all sites relevant to this model, so as to not have to re-load data for the same sites
site_data <- readr::read_csv("https://raw.githubusercontent.com/eco4cast/neon4cast-targets/main/NEON_Field_Site_Metadata_20220412.csv") %>%
  filter(if_any(matches(model_types),~.==1))
all_sites = site_data$field_site_id

#Specify desired met variables
variables <- c('air_temperature')

#Code from Freya Olsson to download and format meteorological data (had to be modified to deal with arrow issue on M1 mac). Major thanks to Freya here!!

# Load stage 2 data
endpoint = "data.ecoforecast.org"
use_bucket <- paste0("neon4cast-drivers/noaa/gefs-v12/stage2/parquet/0/", noaa_date)
use_s3 <- arrow::s3_bucket(use_bucket, endpoint_override = endpoint, anonymous = TRUE)
noaa_future <- arrow::open_dataset(use_s3) |>
  dplyr::collect() |>
  dplyr::filter(site_id %in% all_sites,
                datetime >= forecast_date,
                variable == variables) #It would be more efficient to filter before collecting, but this is not running on my M1 mac

# Load stage3 data. 
#The bucket is somewhat differently organized here, necessitating a different structure
endpoint <- "data.ecoforecast.org"
load_stage3 <- function(site,endpoint,variables){
  message('run ', site)
  use_bucket <- paste0("neon4cast-drivers/noaa/gefs-v12/stage3/parquet/", site)
  use_s3 <- arrow::s3_bucket(use_bucket, endpoint_override = endpoint, anonymous = TRUE)
  parquet_file <- arrow::open_dataset(use_s3) |>
    dplyr::collect() |>
    dplyr::filter(datetime >= lubridate::ymd('2017-01-01'),
                  variable %in% variables) #It would be more efficient to filter before collecting, but this is not running on my M1 mac
}

noaa_past <- map_dfr(all_sites, load_stage3,endpoint,variables)

# Format met forecasts
noaa_future_daily <- noaa_future |> 
  mutate(datetime = lubridate::as_date(datetime)) |> 
  # mean daily forecasts at each site per ensemble
  group_by(datetime, site_id, parameter, variable) |> 
  summarize(prediction = mean(prediction)) |>
  pivot_wider(names_from = variable, values_from = prediction) |>
  # convert to Celsius
  mutate(air_temperature = air_temperature - 273.15) |> 
  select(datetime, site_id, air_temperature, parameter)

# Format historical met data
noaa_past_mean <- noaa_past |> 
  na.omit() |> 
  mutate(datetime = lubridate::as_date(datetime)) |> 
  group_by(datetime, site_id, variable) |> 
  summarize(prediction = mean(prediction, na.rm = TRUE), .groups = "drop") |> 
  pivot_wider(names_from = variable, values_from = prediction) |> 
  # convert air temp to C
  mutate(air_temperature = air_temperature - 273.15)

rm(noaa_past) #Forget this huge file

# Plot met
ggplot(noaa_future_daily, aes(x=datetime, y=air_temperature)) +
  geom_line(aes(group = parameter), alpha = 0.4)+
  geom_line(data = noaa_past_mean, colour = 'darkblue') +
  coord_cartesian(xlim = c(noaa_date - lubridate::days(60),
                           noaa_date + lubridate::days(35)))+
  facet_wrap(~site_id, scales = 'free')



#Step 3.0: Define the forecasts model for a site
forecast_site <- function(site,noaa_past_mean,noaa_future_daily,target_variable) {
  message(paste0("Running site: ", site))
  
  # Get site information for elevation
  site_info <- site_data |> dplyr::filter(field_site_id == site)
  
  # historical temperatures determined in the chunk above. Integrate here?
  
  # Merge in past NOAA data into the targets file, matching by date.
  site_target <- target |>
    dplyr::select(datetime, site_id, variable, observation) |>
    dplyr::filter(variable %in% c(target_variable), 
                  site_id == site) |>
    tidyr::pivot_wider(names_from = "variable", values_from = "observation") |>
    dplyr::left_join(noaa_past_mean%>%
                       filter(site_id == site), by = c("datetime", "site_id"))
  
  if(!target_variable%in%names(site_target)){
    message(paste0("No target observations at site ",site,". Skipping forecasts at this site."))
    return()
    
    } else if(sum(!is.na(site_target$air_temperature)&!is.na(site_target[target_variable]))==0){
    message(paste0("No historical air temp data that corresponds with target observations at site ",site,". Skipping forecasts at this site."))
    return()
      
    } else {
      # Fit linear model based o # n past data: water temperature = m * air temperature + b
      fit <- lm(get(target_variable) ~ air_temperature, data = site_target)
      
      #  Get 30-day predicted temperature ensemble at the site
      noaa_future <- noaa_future_daily%>%
        filter(site_id==site)
      
      # use the linear model (predict.lm) to forecast water temperature for each ensemble member
      forecast <- 
        noaa_future |> 
        mutate(site_id = site,
               prediction = predict(fit, tibble(air_temperature)),
               variable = target_variable)
      
      # Format results to EFI standard
      forecast <- forecast |>
        mutate(reference_datetime = forecast_date,
               family = "ensemble",
               model_id = model_id) |>
        select(model_id, datetime, reference_datetime,
               site_id, family, parameter, variable, prediction)
  }
}

#Quick function to repeat for all variables
run_all_vars = function(var,sites,forecast_site,noaa_past_mean,noaa_future_daily){
  
  message(paste0("Running variable: ", var))
  forecast <- map_dfr(sites,forecast_site,noaa_past_mean,noaa_future_daily,var)
  
}

### AND HERE WE GO! We're ready to start forecasting ### 
for (theme in model_themes) {
  #Step 1: Download latest target data and site description data
  target = download_target(theme)
  type = ifelse(theme%in% c("terrestrial_30min", "terrestrial_daily"),"terrestrial",theme)
  
  site_data <- readr::read_csv("https://raw.githubusercontent.com/eco4cast/neon4cast-targets/main/NEON_Field_Site_Metadata_20220412.csv") %>%
    filter(get(type)==1)
  
  #Set target variables and timesteps
  if(theme == "aquatics")           {vars = c("temperature","oxygen","chla")
                                     horiz = 30
                                     step = 1
                                     }
  if(theme == "ticks")              {vars = c("amblyomma_americanum")
                                     horiz = 52 #52 weeks
                                     step = 7
                                     }
  if(theme == "phenology")          {vars = c("gcc_90","rcc_90")
                                     horiz = 30 
                                     step = 1}
  if(theme == "beetles")            {vars = c("abundance","richness")
                                     horiz = 52
                                     step = 7}
  if(theme == "terrestrial_daily")  {vars = c("nee","le")
                                     horiz = 30
                                     step = 1}
  if(theme == "terrestrial_30min")  {vars = c("nee","le")
                                     horiz = 30
                                     step = 1/24/2}
  ## Test with a single site first!
  forecast <- map_dfr(vars,run_all_vars,sites[23],forecast_site,noaa_past_mean,noaa_future_daily)
  
  #Visualize the ensemble predictions -- what do you think?
  forecast |> 
    ggplot(aes(x = datetime, y = prediction, group = parameter)) +
    geom_line(alpha=0.3) +
    facet_wrap(~variable, scales = "free")
  
  # Run all sites -- may be slow!
  forecast <- map_dfr(vars,run_all_vars,sites,forecast_site,noaa_past_mean,noaa_future_daily)
  
  #Forecast output file name in standards requires for Challenge.
  # csv.gz means that it will be compressed
  file_date <- Sys.Date() #forecast$reference_datetime[1]
  forecast_file <- paste0(theme,"-",file_date,"-",model_id,".csv.gz")
  
  #Write csv to disk
  write_csv(forecast, forecast_file)
}

#metadata_file <- neon4cast::generate_metadata(forecast_file, team_list, model_metadata) #Function is not currently available

# Step 5: Submit forecast!

neon4cast::submit(forecast_file = forecast_file, metadata = NULL, ask = FALSE)
```

Head start code for time series forecasts

#Step 3.0: Generate forecasts for each site
# use explicit NAs for gaps in timeseries
blinded_target <- site_target %>% 
  group_by(time, siteID, variable)%>%
  summarize_all(mean, na.rm = T)%>%
  as_tsibble(index="time", key=c("variable","siteID"))%>%
  fill_gaps(.end = Sys.Date())
